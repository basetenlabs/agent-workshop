{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Baseten agent workshop\n",
        "\n",
        "Welcome to the Baseten Agent Workshop!\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Create a [Baseten account](https://app.baseten.co/signup/)\n",
        "2. Add [DeepSeek V3](https://app.baseten.co/model-apis/create) to your workspace\n",
        "3. Create an [API key](https://app.baseten.co/settings/api_keys) and paste it below\n",
        "\n",
        "Let's build an agent!\n"
      ],
      "metadata": {
        "id": "EPofNbLRksuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Baseten model config\n",
        "\n",
        "API_KEY = \"BASETEN-API-KEY-HERE\" # Paste your API key in this string\n",
        "BASE_URL = \"https://inference.baseten.co/v1\"\n",
        "MODEL_NAME = \"deepseek-ai/DeepSeek-V3\""
      ],
      "metadata": {
        "id": "_ivL6vGyorls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Install dependencies\n",
        "\n",
        "!pip install openai-agents\n",
        "!pip install openai\n",
        "!pip install aiohttp"
      ],
      "metadata": {
        "id": "265fZTYWodVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom model provider\n",
        "\n",
        "import asyncio\n",
        "import random\n",
        "import uuid\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from agents import (\n",
        "    Agent,\n",
        "    HandoffOutputItem,\n",
        "    ItemHelpers,\n",
        "    MessageOutputItem,\n",
        "    RunContextWrapper,\n",
        "    Runner,\n",
        "    ToolCallItem,\n",
        "    ToolCallOutputItem,\n",
        "    TResponseInputItem,\n",
        "    function_tool,\n",
        "    handoff,\n",
        "    trace,\n",
        "    set_tracing_disabled,\n",
        "    RunConfig,\n",
        "    Model,\n",
        "    ModelProvider,\n",
        "    OpenAIChatCompletionsModel,\n",
        ")\n",
        "from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# ✅ Disable Datadog tracing (not needed in Colab)\n",
        "set_tracing_disabled(True)\n",
        "\n",
        "# ✅ Custom model provider using OpenAI-compatible interface\n",
        "client = AsyncOpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
        "\n",
        "class CustomModelProvider(ModelProvider):\n",
        "    def get_model(self, model_name: str | None) -> Model:\n",
        "        return OpenAIChatCompletionsModel(\n",
        "            model=model_name or MODEL_NAME,\n",
        "            openai_client=client\n",
        "        )\n",
        "\n",
        "CUSTOM_PROVIDER = CustomModelProvider()"
      ],
      "metadata": {
        "id": "LsDMFZajo949"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define context and tools\n",
        "\n",
        "# ✅ Context definition\n",
        "class AirlineAgentContext(BaseModel):\n",
        "    passenger_name: str | None = None\n",
        "    confirmation_number: str | None = None\n",
        "    seat_number: str | None = None\n",
        "    flight_number: str | None = None\n",
        "\n",
        "# ✅ Tools\n",
        "@function_tool(name_override=\"faq_lookup_tool\", description_override=\"Lookup frequently asked questions.\")\n",
        "async def faq_lookup_tool(question: str) -> str:\n",
        "    if \"bag\" in question or \"baggage\" in question:\n",
        "        return \"You are allowed to bring one bag on the plane. It must be under 50 pounds and 22 inches x 14 inches x 9 inches.\"\n",
        "    elif \"seats\" in question or \"plane\" in question:\n",
        "        return \"There are 120 seats on the plane. 22 business class, 98 economy. Exit rows are 4 and 16. Economy Plus is 5-8.\"\n",
        "    elif \"wifi\" in question:\n",
        "        return \"We have free wifi on the plane, join Airline-Wifi\"\n",
        "    return \"I'm sorry, I don't know the answer to that question.\"\n",
        "\n",
        "@function_tool\n",
        "async def update_seat(context: RunContextWrapper[AirlineAgentContext], confirmation_number: str, new_seat: str) -> str:\n",
        "    context.context.confirmation_number = confirmation_number\n",
        "    context.context.seat_number = new_seat\n",
        "    assert context.context.flight_number is not None, \"Flight number is required\"\n",
        "    return f\"Updated seat to {new_seat} for confirmation number {confirmation_number}\"\n",
        "\n",
        "# ✅ Handoff logic\n",
        "async def on_seat_booking_handoff(context: RunContextWrapper[AirlineAgentContext]) -> None:\n",
        "    context.context.flight_number = f\"FLT-{random.randint(100, 999)}\""
      ],
      "metadata": {
        "id": "l-3a5d1UpU83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Agent definitions\n",
        "faq_agent = Agent[AirlineAgentContext](\n",
        "    name=\"FAQ Agent\",\n",
        "    handoff_description=\"Answers airline-related questions.\",\n",
        "    instructions=f\"\"\"{RECOMMENDED_PROMPT_PREFIX}\n",
        "You are an FAQ agent. If speaking to a customer, you were likely transferred here.\n",
        "# Routine\n",
        "1. Identify the last question.\n",
        "2. Use the faq lookup tool.\n",
        "3. If unsure, hand back to triage agent.\"\"\",\n",
        "    tools=[faq_lookup_tool],\n",
        ")\n",
        "\n",
        "seat_booking_agent = Agent[AirlineAgentContext](\n",
        "    name=\"Seat Booking Agent\",\n",
        "    handoff_description=\"Can update a passenger's seat.\",\n",
        "    instructions=f\"\"\"{RECOMMENDED_PROMPT_PREFIX}\n",
        "You are a seat booking agent.\n",
        "# Routine\n",
        "1. Ask for confirmation number.\n",
        "2. Ask for desired seat.\n",
        "3. Use the tool to update it.\n",
        "If unsure, hand back to triage agent.\"\"\",\n",
        "    tools=[update_seat],\n",
        ")\n",
        "\n",
        "triage_agent = Agent[AirlineAgentContext](\n",
        "    name=\"Triage Agent\",\n",
        "    handoff_description=\"Routes questions to the correct department.\",\n",
        "    instructions=f\"\"\"{RECOMMENDED_PROMPT_PREFIX}\n",
        "You are a triage agent. Route the user's query to the right specialized agent.\"\"\",\n",
        "    handoffs=[\n",
        "        faq_agent,\n",
        "        handoff(agent=seat_booking_agent, on_handoff=on_seat_booking_handoff),\n",
        "    ],\n",
        ")\n",
        "\n",
        "faq_agent.handoffs.append(triage_agent)\n",
        "seat_booking_agent.handoffs.append(triage_agent)"
      ],
      "metadata": {
        "id": "mFrPuJewpZLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Run the assistant loop\n",
        "async def main():\n",
        "    current_agent: Agent[AirlineAgentContext] = triage_agent\n",
        "    input_items: list[TResponseInputItem] = []\n",
        "    context = AirlineAgentContext()\n",
        "    conversation_id = uuid.uuid4().hex[:16]\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        with trace(\"Customer service\", group_id=conversation_id):\n",
        "            input_items.append({\"content\": user_input, \"role\": \"user\"})\n",
        "            result = await Runner.run(\n",
        "                current_agent,\n",
        "                input_items,\n",
        "                context=context,\n",
        "                run_config=RunConfig(model_provider=CUSTOM_PROVIDER)\n",
        "            )\n",
        "\n",
        "            for new_item in result.new_items:\n",
        "                agent_name = new_item.agent.name\n",
        "                if isinstance(new_item, MessageOutputItem):\n",
        "                    print(f\"{agent_name}: {ItemHelpers.text_message_output(new_item)}\")\n",
        "                elif isinstance(new_item, HandoffOutputItem):\n",
        "                    print(f\"➡️ Handoff from {new_item.source_agent.name} to {new_item.target_agent.name}\")\n",
        "                elif isinstance(new_item, ToolCallItem):\n",
        "                    print(f\"{agent_name}: (Calling a tool...)\")\n",
        "                elif isinstance(new_item, ToolCallOutputItem):\n",
        "                    print(f\"{agent_name}: Tool output → {new_item.output}\")\n",
        "                else:\n",
        "                    print(f\"{agent_name}: (Unhandled output type)\")\n",
        "            input_items = result.to_input_list()\n",
        "            current_agent = result.last_agent\n",
        "\n",
        "# ✅ Run it (you’ll get a text prompt box below the cell)\n",
        "await main()"
      ],
      "metadata": {
        "id": "rBxmSDWOphN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Benchmark 1: Performance test for a single request\n",
        "\n",
        "import asyncio\n",
        "import time\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# ✅ CONFIGURE YOUR ENDPOINT\n",
        "NUM_REQUESTS = 5\n",
        "\n",
        "client = AsyncOpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
        "\n",
        "# ✅ Benchmark 1: Single request - measure tokens per second\n",
        "async def benchmark_single_request(prompt=\"Write a haiku about fireflies.\"):\n",
        "    print(\"🔍 Benchmarking token throughput for a single request...\")\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    start_time = time.perf_counter()\n",
        "    first_token_time = None\n",
        "    token_count = 0\n",
        "\n",
        "    async for chunk in await client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=messages,\n",
        "        stream=True\n",
        "    ):\n",
        "        if len(chunk.choices) > 0:\n",
        "          content = chunk.choices[0].delta.content or \"\"\n",
        "          if content.strip():\n",
        "              if first_token_time is None:\n",
        "                  first_token_time = time.perf_counter()\n",
        "              token_count += 1\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    ttfb = first_token_time - start_time if first_token_time else float('nan')\n",
        "    tps = token_count / (end_time - first_token_time) if first_token_time else 0\n",
        "\n",
        "    print(f\"📥 Time to first token: {ttfb:.2f}s\")\n",
        "    print(f\"📦 Total tokens: {token_count}\")\n",
        "    print(f\"⚡️ Tokens/sec: {tps:.2f}\")\n",
        "    print(f\"⏱ Total time: {total_time:.2f}s\")\n",
        "\n",
        "await benchmark_single_request()"
      ],
      "metadata": {
        "id": "fKCGdC44plEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Benchmark 2: 5 concurrent requests\n",
        "async def send_and_measure(index, prompt=\"Write a haiku about mountains.\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    start_time = time.perf_counter()\n",
        "    first_token_time = None\n",
        "\n",
        "    async for chunk in await client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=messages,\n",
        "        stream=True\n",
        "    ):\n",
        "        if len(chunk.choices) > 0:\n",
        "          content = chunk.choices[0].delta.content or \"\"\n",
        "          if content.strip() and first_token_time is None:\n",
        "              first_token_time = time.perf_counter()\n",
        "              break  # Stop after first token\n",
        "\n",
        "    ttfb = first_token_time - start_time if first_token_time else float('nan')\n",
        "    print(f\"Request {index+1}: ⏱ Time to first token = {ttfb:.2f}s\")\n",
        "    return ttfb\n",
        "\n",
        "async def benchmark_concurrent_requests():\n",
        "    print(f\"\\n🚀 Sending {NUM_REQUESTS} concurrent streaming requests...\")\n",
        "    tasks = [send_and_measure(i) for i in range(NUM_REQUESTS)]\n",
        "    ttfs = await asyncio.gather(*tasks)\n",
        "    avg_ttfb = sum(ttfs) / len(ttfs)\n",
        "    print(f\"\\n📊 Average time to first token: {avg_ttfb:.2f}s\")\n",
        "\n",
        "# ✅ Call it\n",
        "await benchmark_concurrent_requests()"
      ],
      "metadata": {
        "id": "HdbqMJvspp1A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
